{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42f67ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    " \n",
    "<head>\n",
    "  <link rel=\"stylesheet\" type=\"text/css\" href=\".\\Dashboard\\override.css\">\n",
    "</head>\n",
    "<body>\n",
    "    <img src=\".\\Dashboard\\SIMM_header.png\"></img>\n",
    "</body>\n",
    " \n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb9a086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xmltodict\n",
    "import itertools\n",
    "from scipy.stats import norm\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option(\"display.max_rows\", 500, \"display.max_columns\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac2b339",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('static/SIMM Static Data 2.4.xml', 'rb') as fd:\n",
    "    simm_param = xmltodict.parse(fd.read())\n",
    "\n",
    "simm = {}\n",
    "for params in simm_param['StaticData']['SimmStaticData']:\n",
    "    simm[params['ID']]=params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7bce8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities needed to load the static data and manipulate dataframes\n",
    "\n",
    "def reset_index(df, groups):\n",
    "    index_reset = list(range(len(groups)))\n",
    "    return df.reset_index(level=index_reset).groupby(groups)\n",
    "\n",
    "def filter_index(df, index_keys):\n",
    "    df_index = df.index\n",
    "    return df_index.droplevel([level for level in df_index.names if not level in index_keys]).tolist()\n",
    "\n",
    "def flatten(key_prefix, val_prefix, dict_list, transform=lambda x: float(x)):\n",
    "    dd = {}\n",
    "    if isinstance(dict_list, list):\n",
    "        for item in dict_list:\n",
    "            dd.update(flatten(key_prefix, val_prefix, item, transform))\n",
    "        return dd\n",
    "    elif isinstance(dict_list, dict):\n",
    "        key, other = None, {}\n",
    "        for k, v in dict_list.items():\n",
    "            if k.startswith(key_prefix):\n",
    "                key = v if key is None else (key, v)\n",
    "            elif k.startswith(val_prefix):\n",
    "                val = v\n",
    "            else:\n",
    "                other.setdefault(k, flatten(key_prefix, val_prefix, v, transform))\n",
    "        if other:\n",
    "            return {key: other} if key else other\n",
    "        else:\n",
    "            return {key: transform(val)}\n",
    "    else:\n",
    "        return transform(dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d24c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the horizon\n",
    "Horizon = 'Horizon_10d'\n",
    "\n",
    "# class correlations\n",
    "class_rho = {'rho_inter': flatten('@', '#', simm[Horizon]['RiskClassCorrelations']),\n",
    "             # maps risk names to correlation buckets\n",
    "             'risk_names': {'Risk_IRCurve': 'IR', 'Risk_FX': 'FX',\n",
    "                            'Risk_Equity': 'Equity', 'Risk_CreditQ': 'CSR (Q)',\n",
    "                            'Risk_CreditNQ': 'CSR (NQ)', 'Risk_Commodity': 'Commodity'}\n",
    "             }\n",
    "\n",
    "# credit spread risk qualifying constants\n",
    "crq = {'rw': flatten('@bucket', '#', simm[Horizon]['CSRQ']['Weights']),\n",
    "       'con': flatten('@', '#', simm[Horizon]['CSRQ']['ConcentrationThresholds']),\n",
    "       'rho_intra': flatten('@', '#', simm[Horizon]['CSRQ']['IntraCorrelations']),\n",
    "       'rho_inter': flatten('@', '#', simm[Horizon]['CSRQ']['InterCorrelations'])\n",
    "       }\n",
    "\n",
    "# credit spread risk non qualifying constants\n",
    "crnq = {'rw': flatten('@bucket', '#', simm[Horizon]['CSRNQ']['Weights']),\n",
    "        'con': flatten('@', '#', simm[Horizon]['CSRNQ']['ConcentrationThresholds']),\n",
    "        'rho_intra': flatten('@', '#', simm[Horizon]['CSRNQ']['IntraCorrelations']),\n",
    "        'rho_inter': flatten('@', '#', simm[Horizon]['CSRNQ']['InterCorrelations'])\n",
    "        }\n",
    "\n",
    "# ir constants\n",
    "ir = {'rw': flatten('@', '#', simm[Horizon]['IR']['Weights']),\n",
    "      'ccy': flatten('@id', '@vol', simm[Horizon]['IR']['Currencies'], transform=str),\n",
    "      'rho': flatten('@', '#', simm[Horizon]['IR']['IntraCorrelations']),\n",
    "      'rho_inter': float(simm[Horizon]['IR']['InterCorrelation']),\n",
    "      'con': flatten('@', '#', simm[Horizon]['IR']['ConcentrationThresholds']),\n",
    "      'hvr': float(simm[Horizon]['IR']['HistoricalVolatilityRatio']),\n",
    "      'bucket_vol_map': flatten(\n",
    "          '@crif', '@id', simm[Horizon]['IR']['Volatilities']['Weights']['Volatility'], transform=str),\n",
    "      'other_map': {'Risk_Inflation': 'InflationWeight', 'Risk_XCcyBasis': 'CrossCurrencyBasisWeight'}\n",
    "      }\n",
    "\n",
    "# fx constants\n",
    "fx = {'rw': flatten('@', '#', simm[Horizon]['FX']['Weights']),\n",
    "      'ccy': flatten('@id', '@category', simm[Horizon]['FX']['Currencies'], transform=str),\n",
    "      'con': flatten('@', '#', simm[Horizon]['FX']['ConcentrationThresholds']),\n",
    "      'rho': flatten('@', '#', simm[Horizon]['FX']['Correlations']),\n",
    "      \n",
    "      # hardcoded by the ISDA SIMM 2.4 Spec to be High volatility currencies\n",
    "      'high': {'BRL':'2','MXN':'2','TRY':'2','ZAR':'2','ARS':'3'},\n",
    "      'hvr': float(simm[Horizon]['FX']['HistoricalVolatilityRatio'])\n",
    "      }\n",
    "\n",
    "# adjust the fx constants for high volatility currencies \n",
    "for cc, cat in fx['high'].items():\n",
    "    fx['ccy']['Currency'][cc]=cat\n",
    "\n",
    "# equity constants\n",
    "eq = {'rw': flatten('@bucket', '#', simm[Horizon]['Equity']['Weights']),\n",
    "      'con': flatten('@', '#', simm[Horizon]['Equity']['ConcentrationThresholds']),\n",
    "      'rho_intra': flatten('@', '#', simm[Horizon]['Equity']['IntraCorrelations']),\n",
    "      'rho_inter': flatten('@', '#', simm[Horizon]['Equity']['InterCorrelations']),\n",
    "      'hvr': float(simm[Horizon]['Equity']['HistoricalVolatilityRatio'])}\n",
    "\n",
    "# commodity constants\n",
    "cm = {'rw': flatten('@bucket', '#', simm[Horizon]['Commodity']['Weights']),\n",
    "      'con': flatten('@', '#', simm[Horizon]['Commodity']['ConcentrationThresholds']),\n",
    "      'rho_intra': flatten('@', '#', simm[Horizon]['Commodity']['IntraCorrelations']),\n",
    "      'rho_inter': flatten('@', '#', simm[Horizon]['Commodity']['InterCorrelations']),\n",
    "      'hvr': float(simm[Horizon]['Commodity']['HistoricalVolatilityRatio'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e20a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the top level aggregation keys\n",
    "Agg_keys = ['PostRegulations', 'CollectRegulations', 'Counterparty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a2314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_correlation(var, sd, correlation_transform=lambda x: x):\n",
    "    cross = 0.0\n",
    "    for i, j in itertools.combinations(var.keys(), 2):\n",
    "        # default correlations\n",
    "        if i in sd['rho_inter']['Correlation'] and j in sd['rho_inter']['Correlation'][i]['Correlation']:\n",
    "            y_bc = sd['rho_inter']['Correlation'][i]['Correlation'][j]\n",
    "        elif j in sd['rho_inter']['Correlation'] and i in sd['rho_inter']['Correlation'][j]['Correlation']:\n",
    "            y_bc = sd['rho_inter']['Correlation'][j]['Correlation'][i]\n",
    "        cross += 2.0 * correlation_transform(y_bc) * var[i] * var[j]\n",
    "    return cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b1bb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_product(Agg_keys, M, sd):\n",
    "    w = {}\n",
    "    groups = Agg_keys + ['ProductClass']\n",
    "    for group, df in reset_index(M, groups):\n",
    "        margin = {sd['risk_names'][k]: v for k, v in df['Margin'].to_dict().items()}\n",
    "        w[group] = np.sqrt((df['Margin'] * df['Margin']).sum() + apply_correlation(margin, sd))\n",
    "\n",
    "    Simm = pd.DataFrame(w, index=['SIMM']).T\n",
    "    Simm.index.set_names(groups, inplace=True)\n",
    "    return Simm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0421101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_margin_IR(K, sd):\n",
    "    y_bc = sd['rho_inter']\n",
    "    \n",
    "    dm = {}\n",
    "    for group, df in reset_index(K, Agg_keys+['ProductClass', 'RiskType']):\n",
    "        S_b = ((-df['K']).clip(lower=df[['K', 'WS_ki']].min(axis=1))).to_dict()\n",
    "        margin = (df['K'] * df['K']).sum()\n",
    "        for i, j in itertools.combinations(S_b.keys(), 2):\n",
    "            g_bc = min(i[1], j[1]) / max(i[1], j[1])\n",
    "            margin += y_bc * g_bc * 2.0 * S_b[i] * S_b[j]\n",
    "\n",
    "        dm[group[:-1] + (group[-1].replace('Vol', ''), 'Delta')] = np.sqrt(margin)\n",
    "\n",
    "    Margin_IR = pd.DataFrame(dm, index=['Margin']).T\n",
    "    Margin_IR.index.set_names(Agg_keys+['ProductClass', 'RiskType', 'MarginType'], inplace=True)\n",
    "    return Margin_IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcb0eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_margin(K, sd, bucket='WS_k', replace_vol_with=''):\n",
    "    \n",
    "    w = {}\n",
    "    for group, full_df in reset_index(K, Agg_keys+['ProductClass', 'RiskType']):\n",
    "\n",
    "        if 'Residual' in full_df.index:\n",
    "            df = full_df.loc[full_df.index.difference(['Residual'])]\n",
    "            residual = full_df.loc['Residual']['K']\n",
    "        else:\n",
    "            df = full_df\n",
    "            residual = 0.0\n",
    "\n",
    "        S_b = ((-df['K']).clip(lower=df[['K', bucket]].min(axis=1))).to_dict()\n",
    "        group_name = group[:-1] + (group[-1].replace('Vol', replace_vol_with), 'Delta' if bucket == 'WS_k' else 'Vega')\n",
    "        w[group_name] = np.sqrt((df['K'] * df['K']).sum() + apply_correlation(S_b, sd)) + residual\n",
    "\n",
    "    Margin = pd.DataFrame(w, index=['Margin']).T\n",
    "    Margin.index.set_names(Agg_keys+['ProductClass', 'RiskType', 'MarginType'], inplace=True)\n",
    "    return Margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5a29a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_curve_margin(C, sd, lamb, replace_vol_with=''):\n",
    "\n",
    "    w = {}\n",
    "    for group, full_df in reset_index(C, Agg_keys+['ProductClass', 'RiskType']): \n",
    "        margin = 0.0\n",
    "        # keys to lookup lambda\n",
    "        non_resi_key = group + ('NonResidual',)\n",
    "        resi_key = group + ('Residual',)\n",
    "\n",
    "        if 'Residual' in full_df.index:\n",
    "            df = full_df.loc[full_df.index.difference(['Residual'])]\n",
    "            residual = full_df.loc['Residual'][['K', 'CVR_k']]\n",
    "        else:\n",
    "            df = full_df\n",
    "            residual = 0.0\n",
    "\n",
    "        if non_resi_key in lamb:\n",
    "            l = lamb.loc[non_resi_key]\n",
    "            S_b = ((-df['K']).clip(lower=df[['K', 'CVR_k']].min(axis=1))).to_dict()\n",
    "            cm = (df['K'] * df['K']).sum() + apply_correlation(S_b, sd, correlation_transform=lambda x: x*x)\n",
    "            margin += (df['CVR_k'].sum() + l * np.sqrt(cm)).clip(min=0.0)\n",
    "\n",
    "        if resi_key in lamb:\n",
    "            l = lamb.loc[resi_key]\n",
    "            margin += (residual['CVR_k'] + l * residual['K']).clip(min=0.0)\n",
    "\n",
    "        w[group[:-1] + (group[-1].replace('Vol', replace_vol_with), 'Curvature')] = margin\n",
    "\n",
    "    Margin = pd.DataFrame(w, index=['Margin']).T\n",
    "    Margin.index.set_names(Agg_keys+['ProductClass', 'RiskType', 'MarginType'], inplace=True)\n",
    "    return Margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a0adfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_K(filtered, RiskType, sd, reporting_currency='USD', pledgor=False):\n",
    "    index_name = Agg_keys + ['ProductClass', 'RiskType']\n",
    "    # default empty dataframe\n",
    "    delta_margin = pd.DataFrame([], columns=['Margin'], index=pd.MultiIndex(\n",
    "        levels=[[]] * len(index_name), codes=[[]] * len(index_name), names=index_name))\n",
    "\n",
    "    if RiskType == 'Risk_FX':\n",
    "        groups = Agg_keys + ['ProductClass', 'RiskType', 'Qualifier']\n",
    "        s_k_i = filtered.groupby(groups).agg({'Amount': 'sum', 'AmountUSD': 'sum'})\n",
    "        abs_s_k_i = s_k_i.abs()\n",
    "\n",
    "        abs_s_k_i['Concentration_bucket'] = abs_s_k_i.index.get_level_values('Qualifier').map(\n",
    "            lambda ccy: sd['ccy']['Currency'].get(ccy, sd['ccy']['@defaultCategory']))\n",
    "        abs_s_k_i['Concentration_threshold'] = abs_s_k_i['Concentration_bucket'].apply(\n",
    "            lambda bucket: sd['con']['Delta']['Threshold'][bucket])\n",
    "        abs_s_k_i['CR_b'] = np.sqrt(abs_s_k_i['AmountUSD'] / abs_s_k_i['Concentration_threshold']).clip(lower=1.0)\n",
    "\n",
    "        # get high vol or regular risk weights\n",
    "        report_vol = 'High' if reporting_currency in sd['high'] else 'Regular'\n",
    "        rho = sd['rho']['Correlation']['CalculationCurrency{}Volatility'.format(report_vol)]\n",
    "\n",
    "        s_k_i['CR_b'] = abs_s_k_i['CR_b']\n",
    "        s_k_i['RW_k'] = s_k_i.index.get_level_values('Qualifier').map(\n",
    "            lambda ccy: sd['rw']['Delta']['{}{}'.format(\n",
    "                'High' if ccy in sd['high'] else 'Regular', report_vol\n",
    "            )] if ccy != reporting_currency else 0.0)\n",
    "        s_k_i['WS_k_i'] = s_k_i['RW_k'] * s_k_i['AmountUSD'] * s_k_i['CR_b']\n",
    "\n",
    "        w = {}\n",
    "\n",
    "        for group, df in reset_index(s_k_i, Agg_keys + ['ProductClass', 'RiskType']):\n",
    "            ws_ki = df['WS_k_i'].to_dict()\n",
    "            CR_k = df['CR_b'].to_dict()\n",
    "            K_b = sum([x * x for x in ws_ki.values()])\n",
    "\n",
    "            # default correlations\n",
    "            for i, j in itertools.combinations(ws_ki.keys(), 2):\n",
    "                rho_ij = rho['{}{}'.format(\n",
    "                    'High' if i in sd['high'] else 'Regular',\n",
    "                    'High' if j in sd['high'] else 'Regular',\n",
    "                )]\n",
    "                f_kl = min(CR_k[i], CR_k[j]) / max(CR_k[i], CR_k[j])\n",
    "                K_b += 2.0 * f_kl * rho_ij * ws_ki[i] * ws_ki[j]\n",
    "\n",
    "            w[group] = [np.sqrt(K_b), sum(ws_ki.values())]\n",
    "\n",
    "        K = pd.DataFrame(w, index=['K', 'WS_k']).T\n",
    "        K.index.set_names(Agg_keys + ['ProductClass', 'RiskType'], inplace=True)\n",
    "        delta_margin = calc_margin(K, sd)\n",
    "\n",
    "    elif RiskType == 'Risk_IRCurve':\n",
    "        groups = Agg_keys + ['ProductClass', 'RiskType', 'Qualifier', 'Bucket', 'Label1', 'Label2']\n",
    "        other_groups = groups[:-3]\n",
    "        # need to include xccybasis swaps and inflation\n",
    "        other = filtered[filtered['RiskType'].isin(['Risk_XCcyBasis', 'Risk_Inflation'])]\n",
    "        # filter out just Risk_IRCurve\n",
    "        filtered = filtered[filtered['RiskType'] == 'Risk_IRCurve']\n",
    "        s_k_i = filtered.groupby(groups).agg({'Amount': 'sum', 'AmountUSD': 'sum'})\n",
    "\n",
    "        o_k_i = other.groupby(other_groups).agg({'Amount': 'sum', 'AmountUSD': 'sum'})\n",
    "        # first grab any inflation sensitivities\n",
    "        inf = o_k_i.loc[o_k_i.index.get_level_values('RiskType') == 'Risk_Inflation']\n",
    "\n",
    "        # get the concentration thresholds - basically ignore Label 1 and 2 (the tenor and curve index)\n",
    "        sum_ski = s_k_i.groupby(other_groups).agg({'Amount': 'sum', 'AmountUSD': 'sum'})\n",
    "\n",
    "        # add the inflation sensitivities to the sum above - and take absolute value\n",
    "        abs_s_k_i = sum_ski.set_index(sum_ski.index.droplevel('RiskType')).add(\n",
    "            inf.set_index(inf.index.droplevel('RiskType')), fill_value=0.0).abs()\n",
    "\n",
    "        abs_s_k_i['Concentration_bucket'] = abs_s_k_i.index.get_level_values('Qualifier').map(\n",
    "            lambda ccy: sd['ccy']['ConcentrationThresholds']['Currency'].get(\n",
    "                ccy, sd['ccy']['ConcentrationThresholds']['@defaultVolatility']))\n",
    "        abs_s_k_i['Concentration_threshold'] = abs_s_k_i['Concentration_bucket'].apply(\n",
    "            lambda dc_bucket: sd['con']['Delta']['Threshold'][dc_bucket])\n",
    "        abs_s_k_i['CR_b'] = np.sqrt(abs_s_k_i['AmountUSD'] / abs_s_k_i['Concentration_threshold']).clip(lower=1.0)\n",
    "\n",
    "        # copy it to the weighted dataframe\n",
    "        concen = abs_s_k_i['CR_b'].to_dict()\n",
    "        s_k_i['CR_b'] = s_k_i.index.droplevel('RiskType').map(lambda x: concen[x[:-3]])\n",
    "        s_k_i['RW_k'] = [sd['rw']['Delta']['Volatility'][sd['bucket_vol_map'][x[0]]]['Weight'][x[1].lower()]\n",
    "                         for x in filter_index(s_k_i, ['Bucket', 'Label1'])]\n",
    "        s_k_i['WS_k_i'] = s_k_i['RW_k'] * s_k_i['AmountUSD'] * s_k_i['CR_b']\n",
    "\n",
    "        # set the concentration risk factor for inflation risk to what was calculated above\n",
    "        # leave xccy basis swaps at 1.0\n",
    "        o_k_i['CR_b'] = np.where(o_k_i.index.get_level_values('RiskType') == 'Risk_XCcyBasis',\n",
    "                                 1.0, o_k_i.index.droplevel('RiskType').map(lambda x: concen[x]))\n",
    "        o_k_i['RW_k'] = o_k_i.index.map(\n",
    "            lambda x: sd['rw']['Delta']['Volatility'][sd['ccy']['Weights']['Currency'].get(\n",
    "                x[-1], 'High')][sd['other_map'][x[-2]]])\n",
    "\n",
    "        o_k_i['WS_k_i'] = o_k_i['RW_k'] * o_k_i['AmountUSD'] * o_k_i['CR_b']\n",
    "        # group the basis + inflation factors\n",
    "        other_basis = o_k_i.reset_index(\n",
    "            level=[x for x in o_k_i.index.names if x != 'RiskType']).groupby(Agg_keys + ['ProductClass', 'Qualifier'])\n",
    "\n",
    "        # temp dictionary\n",
    "        w = {}\n",
    "\n",
    "        for group, df in reset_index(s_k_i, Agg_keys + ['ProductClass', 'RiskType', 'Qualifier', 'CR_b']):\n",
    "            ws_ki = df['WS_k_i'].to_dict()\n",
    "\n",
    "            # load the xccy basis and inflation\n",
    "            basis_key = group[:-3] + group[-2:-1]\n",
    "            if basis_key in other_basis.groups:\n",
    "                ws_ki.update(other_basis.get_group(basis_key)['WS_k_i'].to_dict())\n",
    "\n",
    "            K_b = sum([x * x for x in ws_ki.values()])\n",
    "\n",
    "            for i, j in itertools.combinations(ws_ki.keys(), 2):\n",
    "                # default correlations\n",
    "                rho_ij = 1.0\n",
    "                phi_ij = 1.0\n",
    "\n",
    "                if i == 'Risk_XCcyBasis' or j == 'Risk_XCcyBasis':\n",
    "                    rho_ij = sd['rho']['CrossCurrencyBasisCorrelation']\n",
    "                elif i == 'Risk_Inflation' or j == 'Risk_Inflation':\n",
    "                    rho_ij = sd['rho']['InflationCorrelation']\n",
    "                else:\n",
    "                    phi_ij = sd['rho']['BasisRiskCorrelation'] if i[1] != j[1] else 1.0\n",
    "                    if i[0] != j[0]:\n",
    "                        if j[0].lower() in sd['rho']['Correlation'] and i[0].lower() in \\\n",
    "                                sd['rho']['Correlation'][j[0].lower()]['Correlation']:\n",
    "                            rho_ij = sd['rho']['Correlation'][j[0].lower()]['Correlation'][i[0].lower()]\n",
    "                        else:\n",
    "                            rho_ij = sd['rho']['Correlation'][i[0].lower()]['Correlation'][j[0].lower()]\n",
    "\n",
    "                K_b += 2.0 * phi_ij * rho_ij * ws_ki[i] * ws_ki[j]\n",
    "\n",
    "            w[group] = [np.sqrt(K_b), sum(ws_ki.values())]\n",
    "\n",
    "        K = pd.DataFrame(w, index=['K', 'WS_ki']).T\n",
    "        K.index.set_names(Agg_keys + ['ProductClass', 'RiskType', 'Qualifier', 'CR_b'], inplace=True)\n",
    "        delta_margin = calc_margin_IR(K, sd)\n",
    "\n",
    "    elif RiskType in ['Risk_CreditNQ', 'Risk_CreditQ', 'Risk_Commodity', 'Risk_Equity']:\n",
    "        groups = Agg_keys + ['ProductClass', 'RiskType', 'Bucket']\n",
    "\n",
    "        credit = RiskType in ['Risk_CreditNQ', 'Risk_CreditQ']\n",
    "        net_sensitivities_by = ['Qualifier', 'Label2'] if credit else ['Qualifier']\n",
    "\n",
    "        # calculate the net sensitivity\n",
    "        s_k_i = filtered.groupby(groups + net_sensitivities_by).agg({'Amount': 'sum', 'AmountUSD': 'sum'})\n",
    "\n",
    "        if s_k_i.empty:\n",
    "            K = pd.DataFrame([], columns=['K', 'WS_k'], index=pd.MultiIndex(\n",
    "                levels=[[]] * len(groups), codes=[[]] * len(groups), names=groups))\n",
    "        else:\n",
    "            abs_s_k_i = s_k_i.abs()\n",
    "            abs_s_k_i['Concentration_threshold'] = abs_s_k_i.index.get_level_values('Bucket').map(\n",
    "                lambda bucket: sd['con']['Delta']['Threshold'][bucket])\n",
    "            abs_s_k_i['CR_b'] = np.sqrt(abs_s_k_i['AmountUSD'] / abs_s_k_i['Concentration_threshold']).clip(lower=1.0)\n",
    "\n",
    "            s_k_i['CR_b'] = abs_s_k_i['CR_b']\n",
    "            s_k_i['RW_k'] = s_k_i.index.get_level_values('Bucket').map(\n",
    "                lambda bucket: sd['rw']['Delta']['Weight'][bucket])\n",
    "            s_k_i['WS_k_i'] = s_k_i['RW_k'] * s_k_i['AmountUSD'] * s_k_i['CR_b']\n",
    "\n",
    "            w = {}\n",
    "            for group, df in s_k_i.reset_index().groupby(groups):\n",
    "                ws_ki = df['WS_k_i'].to_dict()\n",
    "                CR_k = df['CR_b'].to_dict()\n",
    "                q_k = df['Qualifier'].to_dict()\n",
    "\n",
    "                K_b = sum([x * x for x in ws_ki.values()])\n",
    "                for i, j in itertools.combinations(ws_ki.keys(), 2):\n",
    "                    if credit:\n",
    "                        issuer = 'SameName' if q_k[i][:16] == q_k[j][:16] else 'DiffName'\n",
    "                        residual = 'Residual' if group[-1] == 'Residual' else 'NonResidual'\n",
    "                        rho_ij = sd['rho_intra'][residual][issuer]\n",
    "                    else:\n",
    "                        # fetch the correlation for this bucket\n",
    "                        rho_ij = sd['rho_intra']['Correlation'][group[-1]]\n",
    "\n",
    "                    f_kl = min(CR_k[i], CR_k[j]) / max(CR_k[i], CR_k[j])\n",
    "                    K_b += 2.0 * f_kl * rho_ij * ws_ki[i] * ws_ki[j]\n",
    "\n",
    "                w[group] = [np.sqrt(K_b), sum(ws_ki.values())]\n",
    "\n",
    "            K = pd.DataFrame(w, index=['K', 'WS_k']).T\n",
    "            K.index.set_names(groups, inplace=True)\n",
    "            delta_margin = calc_margin(K, sd)\n",
    "\n",
    "    return K, delta_margin, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2901b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vega_K(filtered, RiskType, sd, reporting_currency='USD', pledgor=False):\n",
    "    index_name = Agg_keys + ['ProductClass', 'RiskType']\n",
    "    na = pd.DataFrame([], columns=['Margin'], index=pd.MultiIndex(\n",
    "        levels=[[]] * len(index_name), codes=[[]] * len(index_name), names=index_name))\n",
    "    vega_margin = na\n",
    "    curve_margin = na\n",
    "\n",
    "    alpha = norm.ppf(.99)\n",
    "    filtered['t_kj'] = filtered['Label1'].apply(\n",
    "        lambda x: int(x[:-1]) * ({'d': 1.0, 'w': 7.0, 'm': 365 / 12.0, 'y': 365.0}).get(x.lower()[-1]))\n",
    "    # needed for curvature\n",
    "    filtered['SF_t'] = 0.5 * (14 / filtered['t_kj']).clip(upper=1.0)\n",
    "\n",
    "    if RiskType in ['Risk_IRVol', 'Risk_FXVol', 'Risk_EquityVol', 'Risk_CommodityVol']:\n",
    "        irvol = RiskType == 'Risk_IRVol'\n",
    "        fxvol = RiskType == 'Risk_FXVol'\n",
    "        if fxvol:\n",
    "            groups = Agg_keys + ['ProductClass', 'RiskType', 'Qualifier', 'Label1']\n",
    "            filtered['Sigma_kj'] = filtered['Qualifier'].apply(\n",
    "                lambda ccy: sd['rw']['Delta']['{}{}'.format(\n",
    "                    'High' if ccy[:3] in sd['high'] else 'Regular',\n",
    "                    'High' if ccy[3:] in sd['high'] else 'Regular'\n",
    "                )] * np.sqrt(365.0 / 14) / alpha\n",
    "            )\n",
    "        elif irvol:\n",
    "            groups = Agg_keys + ['ProductClass', 'RiskType', 'Bucket', 'Label1']\n",
    "            # note - Explicitly assuming that the vega amount in the amountUSD column is multiplied by the atm vol\n",
    "            filtered['Sigma_kj'] = 1.0\n",
    "            # explicitly set the Bucket\n",
    "            filtered['Bucket'] = filtered['Qualifier'].map(\n",
    "                lambda ccy: sd['ccy']['ConcentrationThresholds']['Currency'].get(\n",
    "                    ccy, sd['ccy']['ConcentrationThresholds']['@defaultVolatility']))\n",
    "        else:\n",
    "            groups = Agg_keys + ['ProductClass', 'RiskType', 'Qualifier', 'Bucket', 'Label1']\n",
    "            filtered['Sigma_kj'] = filtered['Bucket'].apply(\n",
    "                lambda x: sd['rw']['Delta']['Weight'].get(x) * np.sqrt(365.0 / 14) / alpha)\n",
    "\n",
    "        filtered['Vega_Amount'] = filtered['Sigma_kj'] * filtered['Amount']\n",
    "        filtered['Vega_AmountUSD'] = filtered['Sigma_kj'] * filtered['AmountUSD']\n",
    "\n",
    "        filtered['Curve_Amount'] = filtered['SF_t'] * filtered['Vega_Amount']\n",
    "        filtered['Curve_AmountUSD'] = filtered['SF_t'] * filtered['Vega_AmountUSD']\n",
    "\n",
    "        # curvature margin for irvol is treated differently\n",
    "        hvr, curve_adj = (sd['hvr'], 1.0) if not irvol else (1.0, 1.0 / (sd['hvr'] * sd['hvr']))\n",
    "        VR_ik = hvr * filtered.groupby(groups).agg({'Vega_Amount': 'sum', 'Vega_AmountUSD': 'sum'})\n",
    "        # note we have to flip the sign if this is the pledgor side\n",
    "        CVR_ik = filtered.groupby(groups).agg(\n",
    "            {'Curve_Amount': 'sum', 'Curve_AmountUSD': 'sum'})*(-1.0 if pledgor else 1.0)\n",
    "\n",
    "        if VR_ik.empty:\n",
    "            empty = [[]] * len(groups)\n",
    "            K = pd.DataFrame([], columns=['K', 'WS_k'], index=pd.MultiIndex(\n",
    "                levels=empty, codes=empty, names=groups))\n",
    "        else:\n",
    "            if not irvol:\n",
    "                groups.remove('Label1') \n",
    "                \n",
    "            CVR_k = CVR_ik.groupby(groups).agg({'Curve_Amount': 'sum', 'Curve_AmountUSD': 'sum'})\n",
    "            VR_k = VR_ik.groupby(groups).agg({'Vega_Amount': 'sum', 'Vega_AmountUSD': 'sum'})\n",
    "            abs_VR_k = VR_k.abs()\n",
    "\n",
    "            if fxvol:\n",
    "                abs_VR_k['Concentration_bucket'] = abs_VR_k.index.get_level_values('Qualifier').map(\n",
    "                    lambda ccy: (sd['ccy']['Currency'].get(ccy[:3], sd['ccy']['@defaultCategory']),\n",
    "                                 sd['ccy']['Currency'].get(ccy[3:], sd['ccy']['@defaultCategory']))).to_numpy()\n",
    "                abs_VR_k['Concentration_threshold'] = abs_VR_k['Concentration_bucket'].apply(\n",
    "                    lambda bucket: sd['con']['Vega']['Threshold'][bucket])\n",
    "            else:\n",
    "                abs_VR_k['Concentration_threshold'] = VR_k.index.get_level_values('Bucket').map(\n",
    "                    lambda bucket: sd['con']['Vega']['Threshold'][bucket]).to_numpy()\n",
    "\n",
    "            abs_VR_k['VCR_k'] = np.sqrt(\n",
    "                abs_VR_k['Vega_AmountUSD'] / abs_VR_k['Concentration_threshold']).clip(lower=1.0)\n",
    "            VR_k['VCR_k'] = abs_VR_k['VCR_k']\n",
    "\n",
    "            if fxvol:\n",
    "                VR_k['VRW'] = sd['rw']['Vega']\n",
    "                group_K = Agg_keys + ['ProductClass', 'RiskType']\n",
    "                CVR_k['Is_Residual'] = 'NonResidual'\n",
    "            elif irvol:\n",
    "                VR_k['VRW'] = sd['rw']['Vega']\n",
    "                group_K = Agg_keys + ['ProductClass', 'RiskType', 'Bucket']\n",
    "                CVR_k['Is_Residual'] = 'NonResidual'\n",
    "            else:\n",
    "                VR_k['VRW'] = VR_k.index.get_level_values('Bucket').map(\n",
    "                    lambda b: sd['rw']['Vega']['Weight'].get(b, sd['rw']['Vega']['@defaultWeight']))\n",
    "\n",
    "                group_K = Agg_keys + ['ProductClass', 'RiskType', 'Bucket']\n",
    "                # filter out indices\n",
    "                if RiskType == 'Risk_EquityVol':\n",
    "                    CVR_k = CVR_k.index.get_level_values('Bucket').map(\n",
    "                        lambda x: 1.0 if x != '12' else 0).values.reshape(-1, 1) * CVR_k\n",
    "\n",
    "                # need to separate out residual vs non-residual\n",
    "                CVR_k['Is_Residual'] = CVR_k.index.get_level_values('Bucket').map(\n",
    "                    lambda x: 'Residual' if x == 'Residual' else 'NonResidual')\n",
    "\n",
    "            VR_k['VR_k'] = VR_k['VRW'] * VR_k['Vega_AmountUSD'] * VR_k['VCR_k']\n",
    "\n",
    "            # no concentration thresholds applied - but need to know how to calculate theta and lambda\n",
    "            CVR_k['CVR_k'] = CVR_k['Curve_AmountUSD']\n",
    "            CVR_k['abs_CVR_k'] = CVR_k['CVR_k'].abs()\n",
    "\n",
    "            # group the levels by the length of the aggregation keys plus ['ProductClass', 'RiskType'] (2)\n",
    "            levels = CVR_k.groupby(\n",
    "                group_K[:len(Agg_keys)+2] + ['Is_Residual']).agg({'CVR_k': 'sum', 'abs_CVR_k': 'sum'})\n",
    "            \n",
    "            theta = (levels['CVR_k'] / levels['abs_CVR_k']).clip(upper=0.0)\n",
    "            lamb = (norm.ppf(0.995) ** 2 - 1.0) * (1 + theta) - theta\n",
    "\n",
    "            # first calc Vega\n",
    "            w = {}\n",
    "            for group, df in VR_k.reset_index().groupby(group_K):\n",
    "                # check if this is not ir vol\n",
    "                if not irvol:\n",
    "                    rho_ij = sd['rho']['CorrelationVol'] if fxvol else sd['rho_intra']['Correlation'][group[-1]]\n",
    "                    vr_k = df['VR_k'].to_dict()\n",
    "                    VCR_k = df['VCR_k'].to_dict()\n",
    "                else:\n",
    "                    vr_k = df[['Label1', 'VR_k']].set_index('Label1').to_dict()['VR_k']\n",
    "\n",
    "                K_b = sum([x * x for x in vr_k.values()])\n",
    "\n",
    "                for i, j in itertools.combinations(vr_k.keys(), 2):\n",
    "                    # default correlations\n",
    "                    if irvol:\n",
    "                        rho_ij = sd['rho']['Correlation'][i.lower()]['Correlation'][j.lower()] \\\n",
    "                            if i.lower() in sd['rho']['Correlation'] else sd[\n",
    "                            'rho']['Correlation'][j.lower()]['Correlation'][i.lower()]\n",
    "\n",
    "                    f_kl = 1.0 if irvol else min(VCR_k[i], VCR_k[j]) / max(VCR_k[i], VCR_k[j])\n",
    "                    K_b += 2.0 * f_kl * rho_ij * vr_k[i] * vr_k[j]\n",
    "\n",
    "                w[group] = [np.sqrt(K_b), sum(vr_k.values())]\n",
    "\n",
    "            # now curvature\n",
    "            c = {}\n",
    "            for group, df in CVR_k.reset_index().groupby(group_K):\n",
    "                if not irvol:\n",
    "                    cvr_k = df['CVR_k'].to_dict()\n",
    "                else:\n",
    "                    cvr_k = df[['Label1', 'CVR_k']].set_index('Label1').to_dict()['CVR_k']\n",
    "\n",
    "                K_b = sum([x * x for x in cvr_k.values()])\n",
    "                if not irvol:\n",
    "                    rho2_ij = (sd['rho']['CorrelationVol'] if fxvol else sd['rho_intra']['Correlation'][group[-1]]) ** 2\n",
    "\n",
    "                for i, j in itertools.combinations(cvr_k.keys(), 2):\n",
    "                    if irvol:\n",
    "                        rho2_ij = (sd['rho']['Correlation'][i.lower()]['Correlation'][j.lower()]\n",
    "                                   if i.lower() in sd['rho']['Correlation'] else\n",
    "                                   sd['rho']['Correlation'][j.lower()]['Correlation'][i.lower()]) ** 2\n",
    "\n",
    "                    # note rho2_ij is rho_ij squared\n",
    "                    K_b += 2.0 * rho2_ij * cvr_k[i] * cvr_k[j]\n",
    "\n",
    "                c[group] = [np.sqrt(K_b), sum(cvr_k.values())]\n",
    "\n",
    "            K = pd.DataFrame(w, index=['K', 'VR_k']).T\n",
    "            K.index.set_names(group_K, inplace=True)\n",
    "            vega_margin = calc_margin(K, sd, bucket='VR_k', replace_vol_with='Curve' if irvol else '')\n",
    "\n",
    "            C = pd.DataFrame(c, index=['K', 'CVR_k']).T\n",
    "            C.index.set_names(group_K, inplace=True)\n",
    "            curve_margin = curve_adj * calc_curve_margin(C, sd, lamb, replace_vol_with='Curve' if irvol else '')\n",
    "    else:\n",
    "        if not filtered.empty:\n",
    "            print('todo!')\n",
    "        groups = Agg_keys + ['ProductClass', 'RiskType', 'Qualifier', 'Bucket', 'TradeID']\n",
    "        empty = [[]] * len(groups)\n",
    "        K = pd.DataFrame([], columns=['K', 'WS_k'], index=pd.MultiIndex(\n",
    "            levels=empty, codes=empty, names=groups))\n",
    "        # inflation/credit - TODO!!\n",
    "\n",
    "    return K, vega_margin, curve_margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222dc770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we load a crif file\n",
    "import glob\n",
    "\n",
    "agreementLookup = {'Goldman Sachs Int'               : 'GSIL_TM',\n",
    "                   'Goldman Sachs International ZAR' : 'GSIL_TM',\n",
    "                   'GS Bank'                         : 'GA_Bank_TM',\n",
    "                   'Citibank NA NY'                  : 'Citibank NA_TM',\n",
    "                   'Citigroup G Mkt Ldn'             : 'IB CGMI',\n",
    "                   'Citigroup Global Markets'        : 'CitiGroup_TM',\n",
    "                   'Citibank Jhb'                    : 'Citibank NA_TM',\n",
    "                   'Citibank London'                 : 'Citibank NA_TM',  \n",
    "                   'Citibank NA NY'                  : 'Citibank NA_TM',\n",
    "                   'Soc Gen Jhb'                     : 'SocGen_IMTM',\n",
    "                   'Soc Gen Ldn'                     : 'SocGen_IMTM',\n",
    "                   'Soc Gen Paris'                   : 'SocGen_IMTM',\n",
    "                   'UBS AG London'                   : 'UBS_IMTM',\n",
    "                   'UBS AG Zurich'                   : 'UBS_IMTM'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205d2747",
   "metadata": {},
   "outputs": [],
   "source": [
    "riskgroups = {\n",
    "    'Risk_Equity': (delta_K, eq),\n",
    "    'Risk_Commodity': (delta_K, cm),\n",
    "    'Risk_FX': (delta_K, fx),\n",
    "    'Risk_CreditQ': (delta_K, crq),\n",
    "    'Risk_IRCurve': (delta_K, ir),\n",
    "    'Risk_IRVol': (vega_K, ir),\n",
    "    'Risk_EquityVol': (vega_K, eq),\n",
    "    'Risk_FXVol': (vega_K, fx),\n",
    "    'Risk_CommodityVol': (vega_K, cm)\n",
    "}\n",
    "\n",
    "\n",
    "def report(total, Agg_keys):\n",
    "    IM_x = total.groupby(Agg_keys + ['ProductClass', 'RiskType'])['Margin'].sum()\n",
    "    SIMM_p = calc_product(Agg_keys, IM_x, class_rho)\n",
    "    SIMM = SIMM_p.groupby(Agg_keys).sum('SIMM')\n",
    "\n",
    "    total['Exposure'] = IM_x\n",
    "    total['ProductTotal'] = SIMM_p\n",
    "    total['SIMM_Placed'] = SIMM\n",
    "\n",
    "    total.set_index('Exposure', append=True, inplace=True)\n",
    "    total.set_index('ProductTotal', append=True, inplace=True)\n",
    "    total.set_index('SIMM_Placed', append=True, inplace=True)\n",
    "\n",
    "    return total.reorder_levels(\n",
    "       Agg_keys + ['SIMM_Placed', 'ProductClass', 'ProductTotal', 'RiskType', 'Exposure', 'MarginType'])\n",
    "\n",
    "\n",
    "def calc_simm(crif):\n",
    "    # set the correct groups for the different riskTypes\n",
    "    crif['RiskGroup'] = crif['RiskType'].apply(\n",
    "        lambda x: 'Risk_IRCurve' if x in ['Risk_XCcyBasis', 'Risk_Inflation'] else x)\n",
    "    \n",
    "    index_names = Agg_keys + ['ProductClass', 'RiskType']\n",
    "    total_pledgor = pd.DataFrame([], columns=['Margin'], index=pd.MultiIndex(\n",
    "        levels=[[]] * len(index_names), codes=[[]] * len(index_names), names=index_names))\n",
    "    total_secured = pd.DataFrame([], columns=['Margin'], index=pd.MultiIndex(\n",
    "        levels=[[]] * len(index_names), codes=[[]] * len(index_names), names=index_names))\n",
    "    \n",
    "    for group, df in crif.groupby(['PostRegulations', 'CollectRegulations', 'ProductClass', 'RiskGroup']):\n",
    "        riskgroup = group[-1]\n",
    "        margin_calc, params = riskgroups[riskgroup]\n",
    "        K, m1, m2 = margin_calc(df.copy(), riskgroup, params, pledgor=False)\n",
    "\n",
    "        # m1 is either delta or vega - same for pledgor and secured\n",
    "        total_pledgor = total_pledgor.add(m1, fill_value=0.0)\n",
    "        total_secured = total_secured.add(m1, fill_value=0.0)\n",
    "\n",
    "        if margin_calc == vega_K:\n",
    "            # m2 is set at secured (pledgor=False)\n",
    "            total_secured = total_secured.add(m2, fill_value=0.0)\n",
    "            K, m1, m2 = margin_calc(df, riskgroup, params, pledgor=True)\n",
    "            # m2 is now pledgor\n",
    "            total_pledgor = total_pledgor.add(m2, fill_value=0.0)\n",
    "\n",
    "    secured = report(total_secured.droplevel('PostRegulations'), ['CollectRegulations', 'Counterparty'])\n",
    "    pledgor = report(total_pledgor.droplevel('CollectRegulations'), ['PostRegulations', 'Counterparty'])\n",
    "\n",
    "    return {'Pledgor':pledgor,'Secured':secured}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b22462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipywidgets (used to draw the dashboard) github (also with a link to docs) https://github.com/jupyter-widgets/ipywidgets\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb3d467",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SIMM(object):\n",
    "    def __init__(self, crif_paths, cache):\n",
    "        self.cache    = cache\n",
    "        self.paths    = crif_paths\n",
    "        self.source   = widgets.Dropdown(\n",
    "            options=['Arcaida CRIF', 'Adaptiv CRIF'], \n",
    "            value='Adaptiv CRIF', description='CRIF Source:', disabled=False)\n",
    "        \n",
    "        self.load_dates(self.paths[self.source.value])\n",
    "        \n",
    "        self.active_date = widgets.Select(\n",
    "            options=self.dates, value=self.dates[-1], rows=5, \n",
    "            description='RunDate:', disabled=False, layout = {'width': 'max-content'})\n",
    "        \n",
    "        self.lod         = widgets.RadioButtons(\n",
    "            options=['SIMM', 'ByProduct', 'ByExposure', 'ByMargin'],\n",
    "            layout={'width': 'max-content'},\n",
    "            description='Detail:', disabled=False)        \n",
    "        \n",
    "        self.status = widgets.ToggleButtons(\n",
    "            options = ['Pledgor','Secured'],\n",
    "            description='Simm Type:',\n",
    "            disabled=False,\n",
    "            tooltips=['Description of Pledgor', 'Description of Secured'])\n",
    "        \n",
    "        self.output = widgets.Output()\n",
    "        self.label  = widgets.Label(self.active_date.value)\n",
    "        self.load_crif(self.active_date.value)        \n",
    "        \n",
    "        self.source.observe(self.source_changed, 'value')\n",
    "        self.status.observe(self.status_changed, 'value')\n",
    "        self.active_date.observe(self.crif_changed, 'value')\n",
    "        self.lod.observe(self.lod_changed, 'value')    \n",
    "        # the results\n",
    "        self.panel = widgets.VBox([self.source, self.active_date, self.status, self.lod, self.label, self.output])\n",
    "        \n",
    "    def load_dates(self, path, history=10):\n",
    "        self.alldates    = sorted(glob.glob(path))[-history:]\n",
    "        self.date_lookup = {os.path.split(x)[-1]:x for x in self.alldates}\n",
    "        self.dates       = list(self.date_lookup.keys())\n",
    "        \n",
    "    def load_results(self, lod):\n",
    "        if lod == 'SIMM':\n",
    "            df = self.report.reset_index().iloc[:,:3].drop_duplicates().set_index(self.report.index.names[:1])\n",
    "        elif lod == 'ByProduct':\n",
    "            df = self.report.reset_index().iloc[:,:5].drop_duplicates().set_index(self.report.index.names[:4])\n",
    "        elif lod == 'ByExposure':\n",
    "            df = self.report.reset_index().iloc[:,:7].drop_duplicates().set_index(self.report.index.names[:6])\n",
    "        else:\n",
    "            df = self.report\n",
    "        # save this file\n",
    "        filename = './files/{}_{}_{}.csv'.format(self.label.value.replace('.tsv',''), lod, self.status.value)\n",
    "        if not os.path.isfile( filename ):\n",
    "            df.to_csv(filename)\n",
    "            \n",
    "        download = HTML(\n",
    "            '<a href=\"https://icmjhbmvaisprd3:8888/tree/{}\" title=\"click to download\">Download</a>'.format(filename))\n",
    "        \n",
    "        self.output.clear_output(wait=True)\n",
    "        with self.output:        \n",
    "            display(df)\n",
    "            display(download)\n",
    "\n",
    "    def load_crif(self, date):\n",
    "        if date not in self.cache:\n",
    "            crif = pd.read_csv(self.date_lookup[date], sep='\\t', dtype={'Bucket': str})\n",
    "            if self.source.value.startswith('Arcaida'):\n",
    "                crif['Counterparty'] = crif['PortfolioID']\n",
    "                crif['IMModel'] = crif['ImModel']\n",
    "            else:\n",
    "                crif.loc[:,'Counterparty'] = crif['Counterparty'].apply(lambda x: agreementLookup.get(x,x))\n",
    "            self.cache[date] = calc_simm(crif)\n",
    "            \n",
    "        self.report = self.cache[date][self.status.value]\n",
    "        self.label.value = date\n",
    "        self.load_results(self.lod.value)        \n",
    "        \n",
    "    def lod_changed(self, x):\n",
    "        self.load_results(x['new'])\n",
    "            \n",
    "    def status_changed(self, x):\n",
    "        self.report = self.cache[self.active_date.value][self.status.value]\n",
    "        self.load_results(self.lod.value)\n",
    "            \n",
    "    def source_changed(self, x):\n",
    "        self.load_dates(self.paths[x['new']])\n",
    "        self.active_date.options = self.dates\n",
    "        self.active_date.value = self.dates[-1]\n",
    "            \n",
    "    def crif_changed(self, x):        \n",
    "        self.load_crif(x['new'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2270b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up prod server path\n",
    "from conf import PROD_SIMM\n",
    "\n",
    "cache = {}\n",
    "simm_left = SIMM(\n",
    "    {'Adaptiv CRIF': PROD_SIMM+'\\\\CRIF_20*.tsv', \n",
    "     'Arcaida CRIF': PROD_SIMM+'\\\\CRIF_AcadiaSoft_*.tsv'}, cache)\n",
    "simm_right = SIMM(\n",
    "    {'Adaptiv CRIF': PROD_SIMM+'\\\\CRIF_20*.tsv', \n",
    "     'Arcaida CRIF': PROD_SIMM+'\\\\CRIF_AcadiaSoft_*.tsv'}, cache)\n",
    "\n",
    "# display GUI for Voila\n",
    "display(widgets.HBox([simm_left.panel, simm_right.panel]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
