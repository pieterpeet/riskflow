{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6151f6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/mnt/MarketData/JSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3c63c7",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "## High level\n",
    "- Must model ELGD (Expected Loss Given Default)\n",
    "- Must model risk neutral PD's (assuming the ELGD above)\n",
    "- Must model discounted expected exposure\n",
    "\n",
    "## Modelling\n",
    "- Must model collateral with MTA's/Thresholds correctly (**done**)\n",
    "- Must assume a Margin Period of Risk of 4+N for cleared and SFT's 9+N days for everything else where N is the margining frequency (N=1 for daily) (**done**)\n",
    "- Must use risk neutral (i.e. market implied) drifts and vols for all simulation paths where available (only falling back to historical values if there are none) (**mostly done - needs ICIB IT to maintain new vol surfaces**)\n",
    "- Simulation should allow for non-normal exposure distribution (i.e. fat tails) (**implicitly done**)\n",
    "- Take into account WWR (Wrong Way Risk) i.e. correlation of exposure to PD (**not done**)\n",
    "- Multiplier\n",
    "    - The results of the SACVA calc must be scales with the multiplier (lowest value is 1.0 - can be increased at the regulators discretion)\n",
    "- Delta and Vega must be calculated across all asset classes except for Counterparty Credit Risk (where only Delta is required)\n",
    "    - This means we need to calculate vega risk for all reference credit names (**not done**)\n",
    "\n",
    "## Input Data \n",
    "- Credit spread curves need to be maintained and updated\n",
    "- Buckets for Equities, Commodities and Counterparty mapping needs to be maintained \n",
    "- Counterparties that have a legal relationship to one another need to clearly mentioned (they attract a different correlation coefficient later - **TODO**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19763bb6",
   "metadata": {},
   "source": [
    "# What we need to do to meet the above\n",
    "\n",
    "## Modelling TODO\n",
    "- Need to model credit spreads for all Reference Names (to calculate vega on Credit reference names)\n",
    "    - Need a measure of volatility for all reference names \n",
    "        - can use historical vols of spreads if none available (probably will be the case)\n",
    "        - this would affect names like natwest\n",
    "- WWR\n",
    "    - Can be modelled but is quite hard - need historical PD's to calibrate to (and we've not had a TSDB history of credit spreads since forever)\n",
    "\n",
    "    \n",
    "## Stuff we need to check with the Regulator\n",
    "- Vols for Equities/IR/Commodities/FX are expressed as being relative\n",
    "    - i.e. the sensitivity of the calc is expressed as 1% relative to the current vol and then multiplied by 100 (i.e. / 0.01)\n",
    "    - e.g. if the fx vol is 10%, then the sensitivity is calculated as $100*(CVA_{10.1\\%}-CVA_{10\\%})$ as opposed to $100*(CVA_{11\\%}-CVA_{10\\%})$\n",
    "        - Note that $100*(CVA_{11\\%}-CVA_{10\\%})$ is closer to the definition of the derivative that a relative shift $\\Big(\\frac{f(x+0.01)-f(x)}{0.01}\\Big)\\approx f'(x)$ \n",
    "        - And $100*(CVA_{10.1\\%}-CVA_{10\\%})$ is dependent on the current value of the vol i.e. would be calculated as $100*f'(x)*0.01*x = x*f'(x)$ which, (if vols<100%) is considerably smaller \n",
    "            - happy to do this but maybe let the regulator know that this is not consistent with other measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839558a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libs\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from functools import reduce\n",
    "# need to strip out non-digits or non-ascii\n",
    "from string import digits, ascii_letters\n",
    "# need to read vega file\n",
    "import riskflow as rf\n",
    "# import riskflow.utils as utils\n",
    "# from riskflow.adaptiv import AdaptivContext\n",
    "from riskflow.riskfactors import construct_factor\n",
    "\n",
    "# formatting and setup\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.options.display.max_rows = 200\n",
    "\n",
    "# get all the sensitivities\n",
    "market_data_dir = '/mnt/MarketData/CVAMarketDataBackup'\n",
    "base_dir = '/mnt/xva/SACVA'\n",
    "sensitivities_dir = os.path.join(base_dir, 'Greeks')\n",
    "output_dir = '.'\n",
    "\n",
    "\n",
    "# Change the folder/rundate here\n",
    "with open('/mnt/xva/RunDate.txt', 'rt') as f:\n",
    "    rundate = f.read()\n",
    "    \n",
    "rundate='2024-01-26'\n",
    "print('running SACVA aggregation for {}'.format(rundate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e3c679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the short date\n",
    "short_date = ''.join(rundate[2:].split('-')[::-1])\n",
    "# load the marketdata and the vega marketdata\n",
    "base_md = rf.load_market_data('', market_data_dir, json_name='CVAMarketData_Calibrated_New_{}.json'.format(''.join(short_date)))\n",
    "vega_md = rf.load_market_data('', market_data_dir, json_name='CVAMarketData_Calibrated_Vega_{}.json'.format(''.join(short_date)))\n",
    "\n",
    "\n",
    "if 'HullWhite2FactorModelParameters.USD-OIS' not in base_md.params['Price Factors']:\n",
    "    base_md.params['Price Factors']['HullWhite2FactorModelParameters.USD-OIS'] = base_md.params['Price Factors']['HullWhite2FactorModelParameters.USD-SOFR']\n",
    "    \n",
    "if 'HullWhite2FactorModelParameters.USD-OIS' not in vega_md.params['Price Factors']:\n",
    "    vega_md.params['Price Factors']['HullWhite2FactorModelParameters.USD-OIS'] = vega_md.params['Price Factors']['HullWhite2FactorModelParameters.USD-SOFR']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40066541",
   "metadata": {},
   "source": [
    "## EQ, FX, CM and IR Vega\n",
    "\n",
    "For IR Vega per currency (and Commodity Vega), the change in the calibration parameters is also loaded up and the CVA change is calculated (and then divided by 1%). Note that currently the gradient is calculated based on an absolute shift of 1% implied vol (the regs say 1% relative shift but that sounds wrong)\n",
    "\n",
    "For FX and EQ vega, we already have an ATM GBM vol term structure so we just need to shift it directly by 1% (done later in the apply_vega function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9fa892",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calc_implied_vega_diff(vega_params):\n",
    "    deltas = []\n",
    "    for param in vega_params:\n",
    "        delta = []\n",
    "        index = []\n",
    "        factor = param.split('.')\n",
    "        zero = construct_factor(\n",
    "            rf.utils.Factor(factor[0], (factor[1],)), base_md.params['Price Factors'], base_md.params['Price Factor Interpolation'])\n",
    "        vega = construct_factor(\n",
    "            rf.utils.Factor(factor[0], (factor[1],)), vega_md.params['Price Factors'], vega_md.params['Price Factor Interpolation'])\n",
    "\n",
    "        z = zero.current_value()\n",
    "        v = vega.current_value()\n",
    "        for k,tenor in vega.get_tenor_indices().items():\n",
    "            index.extend(list(zip([k]*len(tenor),tenor.reshape(-1))))\n",
    "            if k in v:\n",
    "                delta.extend(v[k]-z[k])\n",
    "            else:\n",
    "                delta.extend([None])\n",
    "        deltas.append(pd.DataFrame(delta, index=pd.MultiIndex.from_tuples(index), columns=[factor[1]]))\n",
    "    \n",
    "    # now we know how the calibration parameters changed for a 1% bump in implied vols (i.e. impact of calibration)\n",
    "    Vega = pd.concat(deltas, axis=1).T\n",
    "    return Vega.T.to_dict()\n",
    "\n",
    "# grab the vega parameters for IR and CM\n",
    "ir_vega_params = [x for x in vega_md.params['Price Factors'].keys() if x.startswith('HullWhite2FactorModelParameters')]\n",
    "cm_vega_params = [x for x in vega_md.params['Price Factors'].keys() if x.startswith('CSForwardPriceModelParameters')]\n",
    "IR_Vega_Lookup = calc_implied_vega_diff(ir_vega_params)\n",
    "CM_Vega_Lookup = calc_implied_vega_diff(cm_vega_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e29229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the percentage difference in vega parameters for interest rates (should be small)\n",
    "100*pd.DataFrame(IR_Vega_Lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60562c3",
   "metadata": {},
   "source": [
    "## Load up parameters\n",
    "\n",
    "- Load up Risk weights by risk type (FX, Commodity, Counterparty, Equity, Interest Rate)\n",
    "- Load up Correlations for some risk types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc13b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "domestic_ccy = 'ZAR'\n",
    "CR_cpy_correlation = pd.read_csv('static/FRTB_CR_Cpy_Bucket_Correlations.csv', index_col=0)\n",
    "CR_cpy_buckets = pd.read_csv('static/FRTB_CR_Cpy.csv', index_col=1)\n",
    "CR_ref_correlation = pd.read_csv('static/FRTB_CR_Ref_Correlation.csv', index_col=0)\n",
    "CR_ref_buckets = pd.read_csv('static/FRTB_CR_Ref.csv', index_col=0)\n",
    "\n",
    "CM_buckets = pd.read_csv('static/FRTB_CM_Buckets.csv', index_col=0)\n",
    "CM_map = pd.read_csv('static/comm.csv', index_col=0)\n",
    "CM_lookup = CM_map['Class'].to_dict()\n",
    "\n",
    "EQ_buckets = pd.read_csv('static/FRTB_EQ_Buckets.csv', index_col=0)\n",
    "EQ_bucket_map = EQ_buckets['Equity Group'].to_dict()\n",
    "EQ_Reverse_Buckets = EQ_buckets.reset_index().set_index('Equity Group')['Bucket'].to_dict()\n",
    "EQ_map = pd.read_csv('static/SIMM_Buckets.csv')\n",
    "EQ_lookup = EQ_map.set_index(EQ_map['FA Stock Name'].apply(lambda x: x.strip().replace('/', '_')))['SIMM Bucket'].apply(lambda x:EQ_bucket_map[int(x) if not x.startswith(' ') else 11]).to_dict()\n",
    "CR_Credit_Spread = pd.read_csv('static/CP_mapping.csv', index_col=0)\n",
    "\n",
    "IR_Delta_Correlation = pd.read_csv('static/FRTB_IR_Correlation.csv', index_col=0)\n",
    "IR_Delta_RW = pd.read_csv('static/FRTB_IR_RW.csv', index_col=0)\n",
    "IR_Delta_Curr = [domestic_ccy]+pd.read_csv('static/FRTB_IR_RW_CCY.csv').columns.to_list()\n",
    "IR_Delta_Tenors = pd.to_numeric(IR_Delta_RW.index[:-1], errors='ignore').values\n",
    "CR_Delta_Tenors = np.array([.5, 1, 3, 5, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f1baf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_digits = str.maketrans('', '', digits)\n",
    "no_alpha  = str.maketrans('', '', ascii_letters)\n",
    "\n",
    "CR_Credit_Spread = pd.read_csv('static/CP_mapping.csv', index_col=0)\n",
    "\n",
    "CR_bucket_map = {'1a':'Sovereigns_a',\n",
    "                 '1b':'Sovereigns_b',\n",
    "                 '2':'Financials',\n",
    "                 '3':'Industrials',\n",
    "                 '4':'Consumer Goods',\n",
    "                 '5':'Technology',\n",
    "                 '6':'Utilities',\n",
    "                 '7':'Other', \n",
    "                 '8':'QI'}\n",
    "\n",
    "CR_Credit_Spread['Code']=CR_Credit_Spread.apply(lambda x:('IG_' if x['Bucket']=='IG' else 'HYNR_')+CR_bucket_map[x['Reg CVA Bucket']], axis=1)\n",
    "Kudsai = CR_Credit_Spread['Code'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64814f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to interpolate the IR curves at the tenors specified\n",
    "\n",
    "def calc_bucket_sensi(delta_tenors, val):\n",
    "    # old ir bucket calc - uses little sliding triangular shifts to estimate the delta\n",
    "    # has since been superceeded by ir_bucket_sensi (see below)\n",
    "    max_tenor = val.index.max()\n",
    "    deltas = delta_tenors[:delta_tenors.searchsorted(max_tenor)+1]\n",
    "    x = val.index.values\n",
    "    y = val.iloc[:,-1].values\n",
    "    bump = []\n",
    "    prev_tenor = 0.0    \n",
    "    for index, tenor in enumerate(deltas):        \n",
    "        prev_piece = (x>=prev_tenor)&(x<=tenor)\n",
    "        next_tenor = delta_tenors[index+1] if index<delta_tenors.size-1 else max_tenor\n",
    "        next_piece = (x>=tenor)&(x<=next_tenor)\n",
    "        prev_sensi = (np.interp(x[prev_piece], [prev_tenor,tenor],[0.0, 0.0001])*y[prev_piece]).sum()\n",
    "        next_sensi = (np.interp(x[next_piece], [tenor,next_tenor],[0.0001, 0.0])*y[next_piece]).sum()\n",
    "        prev_tenor = tenor\n",
    "        bump.append((prev_sensi+next_sensi)/0.0001)\n",
    "    return bump, deltas\n",
    "\n",
    "def ir_bucket_sensi(delta_tenors, val):\n",
    "    # this is just a straight sum per bucket\n",
    "    max_tenor = val.index.max()\n",
    "    deltas = delta_tenors[:delta_tenors.searchsorted(max_tenor) + 1]\n",
    "    bump = val.groupby(pd.cut(val.index, bins=np.append([0], deltas)), observed=True).sum().values[:, -1]\n",
    "    return bump, deltas\n",
    "\n",
    "def calc_cs01_sensi(cr_tenors, val):\n",
    "    max_tenor = val.index.max()\n",
    "    cs01_clipped = cr_tenors[cr_tenors.index <= max_tenor]\n",
    "    max_effective_cds_tenor = CR_Delta_Tenors.searchsorted(max_tenor) + 1\n",
    "    # can do this because if a tenor is not in val, then it must be zero\n",
    "    bump = cs01_clipped.reindex(val.index).iloc[:,:max_effective_cds_tenor] * val.iloc[:, -1].values.reshape(-1, 1)\n",
    "    return bump.sum() / 0.0001, CR_Delta_Tenors[:max_effective_cds_tenor]\n",
    "    \n",
    "def interpolate_IR_Curves(df, cs01, crb_name):\n",
    "    q = df.set_index(['Rate','Tenor'])\n",
    "    md = []\n",
    "    for rate in q.index.levels[0]:\n",
    "        val = q.loc[rate]\n",
    "        cpy = val.columns[-1]\n",
    "        name = (rate.split('/')[1] if '/' in rate else rate).split('.')\n",
    "        if name[0].startswith('InterestRate') or name[0].startswith('InflationRate'):\n",
    "            if name[1][:3] in IR_Delta_Curr:\n",
    "                bump, ir_deltas = ir_bucket_sensi(IR_Delta_Tenors, val)\n",
    "                r=pd.DataFrame({'Rate':rate, 'Tenor2':0, 'Tenor3':0, cpy:bump}, index=ir_deltas )\n",
    "                r.index.name='Tenor'\n",
    "            else:\n",
    "                # just a straight sum\n",
    "                r=pd.DataFrame({'Rate':rate, 'Tenor2':0, 'Tenor3':0, cpy:val.iloc[:,-1].sum()}, index=[0.0] )\n",
    "                r.index.name='Tenor'\n",
    "                \n",
    "        elif name[0].startswith('SurvivalProb'):\n",
    "            # need to translate the shift in negative log survival Probability\n",
    "            # to a shift in credit spreads (i.e. the cs01)\n",
    "            \n",
    "            bump, cr_deltas = calc_cs01_sensi(cs01, val)\n",
    "            new_rate = 'SurvivalProb.{}'.format(crb_name)\n",
    "            r = pd.DataFrame(\n",
    "                {'Rate': new_rate, 'Tenor2': 0, 'Tenor3': 0, cpy: bump.values},\n",
    "                index=cr_deltas)\n",
    "            r.index.name = 'Tenor'\n",
    "        else:\n",
    "            r = val\n",
    "            r['Rate']=rate\n",
    "\n",
    "        md.append(r)\n",
    "\n",
    "    return pd.concat(md, sort=True).reset_index().set_index(['Rate','Tenor','Tenor2','Tenor3']) if md else df\n",
    "\n",
    "cva_default = pd.read_csv( os.path.join(base_dir, 'Stats','SA_CVA_Stats_{}_Total.csv'.format(rundate)), index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7c8efb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "delta={}\n",
    "for crb in glob.glob(os.path.join(sensitivities_dir, 'SACVA_'+rundate+'*.csv')):\n",
    "    filename = os.path.split(crb)[1]    \n",
    "    crb_name = filename[filename.find('CrB_'):-4]\n",
    "    # if crb_name!='CrB_ACWA_Power_SolarReserve_Redstone_So_NonISDA':\n",
    "    #    continue\n",
    "    cs01_name = 'CS01_{}_{}.csv'.format(rundate, crb_name)\n",
    "    if os.path.isfile(os.path.join(sensitivities_dir, cs01_name)):\n",
    "        cs01 = pd.read_csv(os.path.join(sensitivities_dir, cs01_name), index_col=0, skiprows=1)\n",
    "    else:\n",
    "        cs01 = None\n",
    "    greeks = pd.read_csv(crb).iloc[:,:-1]\n",
    "    try:\n",
    "        delta[crb_name]= interpolate_IR_Curves(greeks, cs01, crb_name)\n",
    "    except:\n",
    "        print('skipping {}'.format(crb) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875c9139",
   "metadata": {},
   "source": [
    "## Tenor Buckets\n",
    "\n",
    "- IR data needs to be bucketed into the following (for Domestic, USD,JPY, EUR, GBP, AUD, CAD, SEK) :\n",
    "  - 1 year\n",
    "  - 2 years\n",
    "  - 5 years\n",
    "  - 10 years\n",
    "  - 30 years\n",
    "  \n",
    "- Counterparty spread data needs the following buckets:\n",
    "  - 6 months\n",
    "  - 1 year\n",
    "  - 3 years\n",
    "  - 5 years\n",
    "  - 10 years\n",
    "  \n",
    "FX, Commodities and Equites do not have tenors  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e418c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_vega(row):\n",
    "    if row.name[0].startswith('HullWhite2FactorModelParameters'):\n",
    "        rate = row.name[0].split('.')\n",
    "        return 100.0 * IR_Vega_Lookup[rate[1]].get((rate[-1], row.name[1]), 0.0) * row[0]\n",
    "\n",
    "    elif row.name[0].startswith('CSForwardPriceModelParameters'):\n",
    "        rate = row.name[0].split('.')\n",
    "        return 100.0 * CM_Vega_Lookup[rate[1]].get((rate[-1], row.name[1]), 0.0) * row[0]\n",
    "\n",
    "    elif row.name[0].startswith('GBMAssetPriceTSModelParameters'):\n",
    "        rate = row.name[0].split('.')\n",
    "        return 100.0 * (0.01 if rate[-1] == 'Vol' else 0.0) * row[0]\n",
    "\n",
    "    else:\n",
    "        return row[0]\n",
    "    \n",
    "def bucket_ir(y):\n",
    "    if y==1:\n",
    "        return '1'\n",
    "    elif y==2:\n",
    "        return '2'\n",
    "    elif y==5:\n",
    "        return '5'\n",
    "    elif y==10:\n",
    "        return '10'\n",
    "    elif y==30:\n",
    "        return '30'\n",
    "    else:\n",
    "        return 'all'\n",
    "    \n",
    "def bucket_cr(y):\n",
    "    if y==.5:\n",
    "        return '6 months'\n",
    "    elif y==1:\n",
    "        return '1 year'\n",
    "    elif y==3:\n",
    "        return '3 year'\n",
    "    elif y==5:\n",
    "        return '5 year'\n",
    "    else:\n",
    "        return '10 year'\n",
    "    \n",
    "def classify(x):\n",
    "    name = x['Rate'].split('.')\n",
    "    if name[0] == 'HullWhite2FactorModelParameters':\n",
    "        return 'Vega.IR.' + name[1][:3]\n",
    "    elif name[0] == 'GBMAssetPriceTSModelParameters':\n",
    "        if len(name[1]) == 3:\n",
    "            return 'Vega.FX.{}.USD'.format(name[1])\n",
    "        else:\n",
    "            eq = name[1].translate(no_digits)\n",
    "            return 'Vega.EQ.' + EQ_lookup.get(eq, 'Other sector')\n",
    "    elif name[0] == 'CSForwardPriceModelParameters':\n",
    "        com = name[1]\n",
    "        return 'Vega.CM.' + CM_lookup[com]\n",
    "    elif name[0] == 'InterestRate':\n",
    "        curr = name[1][:3]\n",
    "        return 'Delta.IR.' + curr + '.' + bucket_ir(x['Tenor'])\n",
    "    elif name[0] == 'InflationRate':\n",
    "        curr = name[1][:3]\n",
    "        return 'Delta.IR.' + curr + '.Inflation'\n",
    "    elif name[0] == 'FxRate':\n",
    "        curr = name[1][:3]\n",
    "        return 'Delta.FX.' + curr + '.USD'\n",
    "    elif name[0] == 'SurvivalProb':\n",
    "        sector = Kudsai.get(name[1], 'HYNR_Other')\n",
    "        return 'Delta.CR.' + sector + '.' + bucket_cr(x['Tenor'])\n",
    "    elif name[0] == 'FXVol':\n",
    "        curr = name[1:]\n",
    "        return 'Vega.FX.' + '.'.join(curr)\n",
    "    elif name[0] == 'InterestRateVol':\n",
    "        curr = name[1][:3]\n",
    "        return 'Vega.IR.' + curr\n",
    "    elif name[0] == 'ForwardPrice':\n",
    "        com = name[1]\n",
    "        return 'Delta.CM.' + CM_lookup[com]\n",
    "    elif name[0] == 'ForwardPriceVol':\n",
    "        com = name[1]\n",
    "        return 'Vega.CM.' + CM_lookup[com]\n",
    "    elif name[0] == 'EquityPrice':\n",
    "        eq = name[1].translate(no_digits)\n",
    "        return 'Delta.EQ.' + EQ_lookup.get(eq, 'Other sector')\n",
    "    elif name[0] == 'EquityPriceVol':\n",
    "        eq = name[1].translate(no_digits)\n",
    "        return 'Vega.EQ.' + EQ_lookup.get(eq, 'Other sector')\n",
    "\n",
    "FRTB_buckets = {}        \n",
    "for k,z in delta.items():\n",
    "    if z.values.shape[0]:\n",
    "        # strip out the fx implied\n",
    "        # v = z[~z.index.get_level_values(0).str.startswith('GBMAssetPriceTSModelParameters')].copy()\n",
    "        v = z.copy()\n",
    "        v.iloc[:,0] = v.apply(apply_vega, axis=1)\n",
    "        v['FRTB_Buckets'] = v.index.to_frame().apply(classify, axis=1)\n",
    "        FRTB_buckets[k]=v.groupby('FRTB_Buckets').agg(np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0cf4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_buckets = pd.concat(FRTB_buckets.values(), axis=1, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a83c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option(\"display.max_rows\", 500, \"display.max_columns\", 20)\n",
    "# all_buckets.to_csv('/mnt/MarketData/ab.csv')\n",
    "all_buckets#['CrB_ACWA_Power_SolarReserve_Redstone_So_NonISDA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368a6d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delta['CrB_ACWA_Power_SolarReserve_Redstone_So_NonISDA']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e40c49",
   "metadata": {},
   "source": [
    "## Vega FX\n",
    "\n",
    "- FX has no term structure with a risk weight of $100\\%$\n",
    "\n",
    "## Vega IR\n",
    "\n",
    "- IR sensitivity is obtained by sensitivity to the Calibration parameters (not the vol surface) - so we need to scale this down\n",
    " - has a risk weight of $100\\%$\n",
    " \n",
    "## Vega CM\n",
    "\n",
    " - CM  has a risk weight of $100\\%$\n",
    " \n",
    "## Vega EQ\n",
    "\n",
    " - EQ has a risk weight of $78\\%$ for large cap buckets and $100\\%$ for everything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c28103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vega_fx = all_buckets.loc[[x for x in all_buckets.index if x.startswith('Vega.FX')]].sum(axis=1)\n",
    "vega_ir = all_buckets.loc[[x for x in all_buckets.index if x.startswith('Vega.IR')]].sum(axis=1)\n",
    "vega_cm = all_buckets.loc[[x for x in all_buckets.index if x.startswith('Vega.CM')]].sum(axis=1)\n",
    "vega_eq = all_buckets.loc[[x for x in all_buckets.index if x.startswith('Vega.EQ')]].sum(axis=1)\n",
    "\n",
    "# constants as laid out above\n",
    "R = 0.01 # Hedge disallowance\n",
    "vega_IR_RW = 1.0\n",
    "vega_FX_RW = 1.0\n",
    "vega_CM_RW = 1.0\n",
    "vega_EQ_L_RW = .78\n",
    "vega_EQ_S_RW = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73af7cd9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vega_ir_crb = all_buckets.loc[[x for x in all_buckets.index if x.startswith('Vega.IR')]].sum(axis=0)\n",
    "vega_ir_crb.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b9b119",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_vega_ir = pd.DataFrame({'SA-WS (CVA)':vega_IR_RW*vega_ir, 'SA-WS (Hedge)':vega_IR_RW*0.0}).set_index(vega_ir.index.map(lambda x:x.split('.',2)[2]))\n",
    "bucket_vega_ir['SA-WS']=bucket_vega_ir['SA-WS (CVA)']+bucket_vega_ir['SA-WS (Hedge)']\n",
    "bucket_vega_ir['K_b']=np.sqrt(bucket_vega_ir['SA-WS']**2 + R*(bucket_vega_ir['SA-WS (Hedge)']**2))\n",
    "bucket_vega_ir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370d4255",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_vega_fx = pd.DataFrame({'SA-WS (CVA)':vega_FX_RW*vega_fx, 'SA-WS (Hedge)':vega_FX_RW*0.0}).set_index(vega_fx.index.map(lambda x:x.split('.',2)[2]))\n",
    "bucket_vega_fx['SA-WS']=bucket_vega_fx['SA-WS (CVA)']+bucket_vega_fx['SA-WS (Hedge)']\n",
    "bucket_vega_fx['K_b']=np.sqrt(bucket_vega_fx['SA-WS']**2 + R*(bucket_vega_fx['SA-WS (Hedge)']**2))\n",
    "bucket_vega_fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf485e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_vega_cm = pd.DataFrame({'SA-WS (CVA)':vega_CM_RW*vega_cm, 'SA-WS (Hedge)':vega_CM_RW*0.0}).set_index(vega_cm.index.map(lambda x:x.split('.',2)[2]))\n",
    "bucket_vega_cm['SA-WS']=bucket_vega_cm['SA-WS (CVA)']+bucket_vega_cm['SA-WS (Hedge)']\n",
    "bucket_vega_cm['K_b']=np.sqrt(bucket_vega_cm['SA-WS']**2 + R*(bucket_vega_cm['SA-WS (Hedge)']**2))\n",
    "bucket_vega_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73d4bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vega_EQ_RW = vega_eq.index.map(lambda x:vega_EQ_L_RW if x.split('.')[2].startswith('L') else vega_EQ_S_RW)\n",
    "bucket_vega_eq = pd.DataFrame({'SA-WS (CVA)':vega_EQ_RW*vega_eq, \n",
    "                               'Risk Weight':vega_EQ_RW,\n",
    "                               'SA-WS (Hedge)':vega_EQ_RW*0.0}).set_index(vega_eq.index.map(lambda x:x.split('.',2)[2]))\n",
    "bucket_vega_eq['SA-WS']=bucket_vega_eq['SA-WS (CVA)']+bucket_vega_eq['SA-WS (Hedge)']\n",
    "bucket_vega_eq['K_b']=np.sqrt(bucket_vega_eq['SA-WS']**2 + R*(bucket_vega_eq['SA-WS (Hedge)']**2))\n",
    "bucket_vega_eq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f056bb66",
   "metadata": {},
   "source": [
    "##  Delta\n",
    "\n",
    "Now calculate all the delta contributions for each risk factor type\n",
    "\n",
    "\n",
    "###  Commodity Delta\n",
    "\n",
    "- All CM Need to be bucketed as per the regs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da365282",
   "metadata": {},
   "outputs": [],
   "source": [
    "CM_RW = CM_buckets.set_index('Commodity Group').to_dict()['Risk Weight']\n",
    "delta_cm = pd.DataFrame({'Delta_CM':all_buckets.loc[[x for x in all_buckets.index if x.startswith('Delta.CM')]].sum(axis=1)})\n",
    "delta_cm['RiskWeight'] = delta_cm.apply(lambda x:CM_RW[x.name.split('.')[2]], axis=1)\n",
    "\n",
    "# todo - add hedge\n",
    "delta_cm['SA-WS (Hedge)'] = 0.0\n",
    "delta_cm['SA-WS (CVA)'] = delta_cm['RiskWeight']*delta_cm['Delta_CM']\n",
    "delta_cm['SA-WS'] = delta_cm['SA-WS (CVA)']+delta_cm['SA-WS (Hedge)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1035722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_delta_cm = delta_cm.set_index(delta_cm.index.map(lambda x:x.split('.',2)[2]))\n",
    "bucket_delta_cm['K_b']=np.sqrt(bucket_delta_cm['SA-WS']**2 + R*(bucket_delta_cm['SA-WS (Hedge)']**2))\n",
    "bucket_delta_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e241cd6f",
   "metadata": {},
   "source": [
    "###  Equity Delta\n",
    "\n",
    "- All EQ Need to be bucketed as per the regs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a39eb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_RW = EQ_buckets.set_index('Equity Group').to_dict()['Risk Weight']\n",
    "delta_eq = pd.DataFrame({'Delta_EQ':all_buckets.loc[[x for x in all_buckets.index if x.startswith('Delta.EQ')]].sum(axis=1)})\n",
    "delta_eq['RiskWeight'] = delta_eq.apply(lambda x:EQ_RW[x.name.split('.')[2]], axis=1)\n",
    "delta_eq['SA-WS (Hedge)'] = 0.0\n",
    "delta_eq['SA-WS (CVA)'] = delta_eq['RiskWeight']*delta_eq['Delta_EQ']\n",
    "delta_eq['SA-WS'] = delta_eq['SA-WS (CVA)']+delta_eq['SA-WS (Hedge)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba5e096",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_delta_eq = delta_eq.set_index(delta_eq.index.map(lambda x:x.split('.',2)[2]))\n",
    "bucket_delta_eq['K_b']=np.sqrt((1-0.01)*bucket_delta_eq['SA-WS']**2 +\n",
    "                               0.01*(bucket_delta_eq['SA-WS (Hedge)']**2+bucket_delta_eq['SA-WS (CVA)']**2))\n",
    "bucket_delta_eq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadef387",
   "metadata": {},
   "source": [
    "###  FX Delta\n",
    "\n",
    "- All fx delta's have the same risk weight (.11)\n",
    "- bucketed by currency pair\n",
    "- must be relative to the reporting currency (here ZAR)\n",
    "- based off a 1% relative shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10d3c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_fx = pd.DataFrame({'Delta_FX':all_buckets.loc[[x for x in all_buckets.index if x.startswith('Delta.FX')]].sum(axis=1)})\n",
    "delta_fx['RiskWeight']=.11\n",
    "delta_fx['SA-WS (Hedge)']=0.0\n",
    "delta_fx['SA-WS (CVA)']=delta_fx['RiskWeight']*delta_fx['Delta_FX']\n",
    "delta_fx['SA-WS']=delta_fx['SA-WS (CVA)']+delta_fx['SA-WS (Hedge)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e2ca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_delta_fx = delta_fx.set_index(delta_fx.index.map(lambda x:x.split('.',2)[2]))\n",
    "bucket_delta_fx['K_b']=np.sqrt(bucket_delta_fx['SA-WS']**2 + R*(bucket_delta_fx['SA-WS (Hedge)']**2))\n",
    "bucket_delta_fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c500d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_cr['CrB_ACWA_Power_SolarReserve_Redstone_So_NonISDA']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1045087c",
   "metadata": {},
   "source": [
    "## Counterparty Delta\n",
    "\n",
    "- need to group counterparties into the following buckets:\n",
    "  - HYNR_Consumer Goods\n",
    "  - HYNR_Financials\t\n",
    "  - HYNR_Industrials\n",
    "  - HYNR_Sovereigns\t(a and b)\n",
    "  - HYNR_Technology\t\n",
    "  - HYNR_Utilities\t\n",
    "  - HYNR Other\n",
    "  - IG_Consumer Goods\n",
    "  - IG_Financials\t\n",
    "  - IG_Industrials\t\n",
    "  - IG_Sovereigns (a and b)\n",
    "  - IG_Technology\t\n",
    "  - IG_Utilities\t\n",
    "  - IG Other\n",
    "\n",
    "- Then need to apply correlation within tenors (for buckets 1-7):\n",
    "    - $p_{tenor}$ is 100% if two tenors are the same, 90% otherwise\n",
    "    - $p_{name}$ is 100% if two names are the same, 90% if distint but legally related (TODO), 50% otherwise\n",
    "    - $p_{quality}$ is 100% if two names have the same quality (IG/IG or HYNR/HYNR), 80% otherwise\n",
    "    - $p_{kl}=p_{tenor}*p_{name}*p_{quality}$ \n",
    "    \n",
    "- currently ignoring legally related entities and bucket 8\n",
    "\n",
    "- Then apply the Hedge disallowance parameter (0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2b957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_cr = all_buckets.loc[[x for x in all_buckets.index if x.startswith('Delta.CR')]]\n",
    "\n",
    "def CR_RW(row):\n",
    "    bucket = row.name.split('.')[2]\n",
    "    return CR_cpy_buckets.loc[bucket]['Risk Weight (RW)']\n",
    "    \n",
    "def CR_reindex(df):\n",
    "    return df.set_index(\n",
    "        pd.MultiIndex.from_arrays(\n",
    "            [df.index.map(lambda x:x.split('.')[2]),df.index.map(lambda x:'.'.join(x.split('.')[3:]))]))\n",
    "    \n",
    "CR_SA_WS_CVA = delta_cr.apply(CR_RW, axis=1).values.reshape(-1,1)*delta_cr\n",
    "CR_SA_WS_hedge = 0.0 * delta_cr\n",
    "CR_SA_WS = CR_SA_WS_CVA+CR_SA_WS_hedge\n",
    "\n",
    "delta_cr_index=CR_reindex(CR_SA_WS)\n",
    "delta_cr_hedge_index=CR_reindex(CR_SA_WS_hedge)\n",
    "delta_cr_CVA_index=CR_reindex(CR_SA_WS_CVA)\n",
    "\n",
    "# get the bucket maps\n",
    "cp_buckets_map = {}\n",
    "for i in delta_cr_index.index.levels[0]:\n",
    "    cp_buckets_map.setdefault(i.split('_',1)[1],[]).append(i)\n",
    "\n",
    "all_delta_cr = {}\n",
    "\n",
    "for CP_group, CP_Classifications in cp_buckets_map.items():\n",
    "    group0 = delta_cr_index.xs(CP_Classifications[0]).dropna(how='all', axis=1)\n",
    "    \n",
    "    if len(CP_Classifications)>1:\n",
    "        group1=delta_cr_index.xs(CP_Classifications[1]).dropna(how='all', axis=1)\n",
    "        tenor_index = np.union1d(group0.index, group1.index)\n",
    "        delta_cp_size = np.array([group0.reindex(tenor_index).size,group1.reindex(tenor_index).size])//tenor_index.size\n",
    "    else:        \n",
    "        tenor_index = group0.index\n",
    "        delta_cp_size = np.array([group0.size])//tenor_index.size\n",
    "        \n",
    "    corr_cp      = []\n",
    "    del_cp       = []\n",
    "    del_cp_cva   = []\n",
    "    del_cp_hedge = []\n",
    "    correlation  = []\n",
    "    \n",
    "    for index, CP_Classification in enumerate(CP_Classifications):\n",
    "        tenor_size = tenor_index.size\n",
    "        delta_cp = delta_cr_index.xs(CP_Classification).dropna(\n",
    "            how='all', axis=1).reindex(tenor_index).fillna(0.0).values.reshape(-1,1)\n",
    "        delta_cp_hedge = delta_cr_hedge_index.xs(CP_Classification).dropna(\n",
    "            how='all', axis=1).reindex(tenor_index).fillna(0.0).values.reshape(-1,1)\n",
    "        delta_cp_cva = delta_cr_CVA_index.xs(CP_Classification).dropna(\n",
    "            how='all', axis=1).reindex(tenor_index).fillna(0.0).values.reshape(-1,1)\n",
    "        \n",
    "        del_cp.append(delta_cp)\n",
    "        del_cp_hedge.append(delta_cp_hedge)\n",
    "        del_cp_cva.append(delta_cp_cva)\n",
    "\n",
    "    # matrix for diagonal\n",
    "    diag = np.ones((tenor_size,tenor_size)) * .9\n",
    "    np.fill_diagonal(diag,1)\n",
    "\n",
    "    # matrix for off diagonal - same Group\n",
    "    off_diag_same = np.ones((tenor_size,tenor_size)) * .9 * .5\n",
    "    np.fill_diagonal(off_diag_same, .5)\n",
    "\n",
    "    # matrix for off diagonal - different Group\n",
    "    off_diag_other = np.ones((tenor_size,tenor_size)) * .9 * .5 * .8\n",
    "    np.fill_diagonal(off_diag_other, .5 * .8)\n",
    "\n",
    "    for i in range(sum(delta_cp_size)):\n",
    "        rows = [diag if j==i else (off_diag_same if i<delta_cp_size[0] else off_diag_other) for j in range(sum(delta_cp_size))]\n",
    "        correlation.append(np.concatenate(rows, axis=1))\n",
    "        \n",
    "    corr = np.concatenate(correlation, axis=0)\n",
    "    delta_cp_full = np.concatenate(del_cp,axis=0)\n",
    "    delta_cp_cva_full = np.concatenate(del_cp_cva,axis=0)\n",
    "    delta_cp_hedge_full = np.concatenate(del_cp_hedge,axis=0)\n",
    "    \n",
    "    weight = delta_cp_full.T.dot(corr).dot(delta_cp_full)\n",
    "\n",
    "    all_delta_cr[CP_group]={'WS_k(CVA)^2' : (delta_cp_cva_full*delta_cp_cva_full).sum(),\n",
    "                            'WS_k(Hdg)^2' : (delta_cp_hedge_full*delta_cp_hedge_full).sum(),\n",
    "                            'WS_k' : delta_cp_full.sum(),\n",
    "                            'WS_k^2' : weight[0][0]}\n",
    "                                     \n",
    "        \n",
    "# make sure the index matches the correlation matrix                                    \n",
    "def CR_Bucket(row):\n",
    "    bucket = row.name\n",
    "    return int(CR_cpy_buckets.loc['IG_'+bucket]['Bucket'].translate(no_alpha))\n",
    "\n",
    "bucket_delta = pd.DataFrame(all_delta_cr).T\n",
    "bucket_delta_cr = bucket_delta.set_index(bucket_delta.apply(CR_Bucket, axis=1))\n",
    "\n",
    "# bucket_delta_cr = pd.DataFrame(all_delta_cr).T.reindex(CR_correlation.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1981e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_delta_cr['Hedge disallowance(R)']=0.01\n",
    "bucket_delta_cr['K_b']=np.sqrt(\n",
    "    bucket_delta_cr['WS_k^2']*( \n",
    "        1-bucket_delta_cr['Hedge disallowance(R)'])+bucket_delta_cr['Hedge disallowance(R)']*(\n",
    "        bucket_delta_cr['WS_k(CVA)^2']+bucket_delta_cr['WS_k(Hdg)^2'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8734c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_delta_cr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcc352a",
   "metadata": {},
   "source": [
    "## Interest Rate Delta\n",
    "\n",
    "- need to group Interest Rates by Currency\n",
    "- need to then apply correct Risk weight for ZAR (Domestic), USD, JPY, EUR, GBP, AUD and CAD curves\n",
    "- once we have the risk weights, need to allow for correlation between different tenors\n",
    "- Then apply the Hedge disallowance parameter (0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c74652",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_ir = pd.DataFrame({'Delta_IR':all_buckets.loc[[x for x in all_buckets.index if x.startswith('Delta.IR')]].sum(axis=1)})\n",
    "\n",
    "#IR_Delta_RW\n",
    "def RW(row):\n",
    "    curr, bucket = row.name.split('.')[-2:]\n",
    "    if curr in IR_Delta_Curr:\n",
    "        return IR_Delta_RW.loc[bucket]['Risk Weight']\n",
    "    else:\n",
    "        return 0.0158\n",
    "    \n",
    "delta_ir['RiskWeight'] = delta_ir.apply(RW, axis=1)\n",
    "delta_ir['SA-WS(CVA)'] = delta_ir['RiskWeight'] * delta_ir['Delta_IR']\n",
    "delta_ir['SA-WS(Hedge)'] = delta_ir['RiskWeight'] * 0.0\n",
    "delta_ir['SA-WS'] = delta_ir['SA-WS(CVA)']+delta_ir['SA-WS(Hedge)']\n",
    "\n",
    "delta_ir_index=delta_ir.set_index(pd.MultiIndex.from_arrays([delta_ir.index.map(lambda x:x.split('.')[2]),delta_ir.index.map(lambda x:x.split('.')[3])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aff088",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_ir\n",
    "#IR_Delta_RW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36d05de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the correlations in IR_Delta_Correlation\n",
    "delta_ir_ws = {}\n",
    "n=IR_Delta_Correlation.index.size\n",
    "for k,v in delta_ir_index.groupby(level=0):\n",
    "    delta_ir_ws[k]={'WS_k(CVA)^2':(v['SA-WS(CVA)']**2).sum(),\n",
    "                    'WS_k(Hdg)^2':(v['SA-WS(Hedge)']**2).sum(),\n",
    "                    'WS_k':v['SA-WS'].sum()}\n",
    "    \n",
    "    if k in IR_Delta_Curr:\n",
    "        block = v.reindex(pd.MultiIndex.from_arrays([[k]*n,IR_Delta_Correlation.index])).fillna(0.0)\n",
    "        WS = block['SA-WS'].values.reshape(-1,1).dot(block['SA-WS'].values.reshape(1,-1))*IR_Delta_Correlation.values        \n",
    "        delta_ir_ws[k]['WS_k^2'] = WS.sum()\n",
    "    else:\n",
    "        delta_ir_ws[k]['WS_k^2'] = (v['SA-WS']**2).sum()\n",
    "        \n",
    "bucket_delta_IR = pd.DataFrame(delta_ir_ws).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d649c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_delta_IR['K_b']=np.sqrt(bucket_delta_IR['WS_k^2']+R*bucket_delta_IR['WS_k(Hdg)^2'])\n",
    "\n",
    "bucket_delta_IR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b567d7",
   "metadata": {},
   "source": [
    "## RiskType cross bucket correlation\n",
    "\n",
    "Amounts should be aggregated across buckets within each risk class. The correlation\n",
    "parameters $\\gamma_{bc}$ applicable to each risk class are set out below\n",
    "\n",
    " - FX is .6 \n",
    " - IR is .5.  \n",
    " - Credit correlation is specifed in the regs\n",
    " - EQ is .15 for buckets 1-10, \n",
    "     - .75 for buckets 12 and 13\n",
    "     - .45 for buckets 12-13 and buckets 1-10\n",
    "     - 0 for bucket 11 and anything else\n",
    " - CM is .2 for buckets 1-10, 0 otherwise\n",
    " \n",
    "\n",
    "\n",
    "$$ Capital_{risktype}=m_{cva}*\\sqrt{\\sum_b K_b^2+\\sum_b \\sum_{c\\neq b}\\gamma_{bc}S_b S_c}$$\n",
    "\n",
    "Where\n",
    "$$S_b=max\\big(min\\big(\\sum_{k\\in b} WS_k, K_b\\big),-K_b\\big)$$\n",
    "$$m_{cva}=1.25$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf44f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def eq_correlation(b,c):\n",
    "    EQ_Reverse_Buckets\n",
    "    b1=min(EQ_Reverse_Buckets[b], EQ_Reverse_Buckets[c])\n",
    "    b2=max(EQ_Reverse_Buckets[b], EQ_Reverse_Buckets[c])\n",
    "    if b1<11 and b2<11:\n",
    "        return .15\n",
    "    if b1==12 and b2==13:\n",
    "        return .75\n",
    "    elif b1<=10 and b2>11:\n",
    "        return .45\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def apply_correlation(var, corr_fn):\n",
    "    cross = 0.0\n",
    "    for i, j in itertools.combinations(var.keys(), 2):\n",
    "        # default correlations\n",
    "        y_bc = corr_fn(i,j)\n",
    "        cross += 2.0 * y_bc * var[i] * var[j]\n",
    "    return cross\n",
    "\n",
    "def calc_margin(df, corr, bucket='WS_k'):\n",
    "    S_b = ((-df['K_b']).clip(lower=df[['K_b', bucket]].min(axis=1))).to_dict()\n",
    "    w = np.sqrt((df['K_b'] * df['K_b']).sum() + apply_correlation(S_b, corr))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d76dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MCVA = 1.25\n",
    "\n",
    "Delta_Risk = pd.DataFrame( {\n",
    "    'FX':MCVA*calc_margin(bucket_delta_fx, lambda b, c:.6, bucket='SA-WS'),\n",
    "    'IR':MCVA*calc_margin(bucket_delta_IR, lambda b, c:.5),\n",
    "    'EQ':MCVA*calc_margin(bucket_delta_eq, eq_correlation, bucket='SA-WS'),\n",
    "    'CM':MCVA*calc_margin(bucket_delta_cm,  lambda b, c:.2, bucket='SA-WS'),\n",
    "    'CR':MCVA*calc_margin(bucket_delta_cr, lambda b, c:CR_cpy_correlation.loc[b][c])\n",
    "    }, index=['Delta'] )\n",
    "\n",
    "Vega_Risk =  pd.DataFrame( {\n",
    "    'FX':MCVA*calc_margin(bucket_vega_fx, lambda b, c:.6, bucket='SA-WS'),\n",
    "    'IR':MCVA*calc_margin(bucket_vega_ir, lambda b, c:.5, bucket='SA-WS'),\n",
    "    'EQ':MCVA*calc_margin(bucket_vega_eq, eq_correlation, bucket='SA-WS'),\n",
    "    'CM':MCVA*calc_margin(bucket_vega_cm,  lambda b, c:.2, bucket='SA-WS')\n",
    "    }, index=['Vega'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944b6664",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final = pd.concat([Delta_Risk, Vega_Risk], sort=False).T.fillna(0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e51c307",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7ab8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978b76ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Final.sum(axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4407d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final.sum(axis=1).sum()*1.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e8332e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final.to_csv('/mnt/MarketData/sensitivities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f593bd8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
